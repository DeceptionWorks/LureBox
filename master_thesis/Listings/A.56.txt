input {
        # use filebeat as input for this log source
        beats {
                port => 5044
                ssl => false
        }
}
filter {
        # add information about the origin of the log source
        mutate { add_field => ["[log_source][received_at]", "%{@timestamp}"] }
        mutate { add_field => ["[log_source][host]", "192.168.56.3"] }
        mutate { add_field => ["[log_source][product]", "PostgreSQL 9.5.6"] }

        # split the CSV message in it's various fields
        csv {
                columns => ["log_time","user_name","database_name","process_id", "connection_from","session_id","session_line_num", "command_tag","session_start_time","virtual_transaction_id", "transaction_id","error_severity","sql_state_code","message", "detail","hint","internal_query","internal_query_pos", "context","query","query_pos","location","application_name"]
                separator => ","
        }

        # split up source IP und source port
       grok {
                match => { "connection_from" => "%{IP:srcip}:%{INT:srcport}" }
        }
        # parse the SQL statements out of the message field
        if ([message] =~ /statement:.*/) {
                grok {
                        match => { "message" => "statement: %{GREEDYDATA:sql_text}" }
                }
                mutate { rename => {"[sql_text]" => "[audit][query][text]"} }
        }

        # summarize fields about the source
        mutate { rename => {"[srcip]" => "[src][ip]"} }
        mutate { rename => {"[srcport]" => "[src][port]"} }
        mutate { rename => {"[application_name]" => "[src][client_program]"} }

        # summarize fields about the audit
        mutate { rename => {"[session_id]" => "[audit][session_id]"} }
        mutate { rename => {"[context]" => "[audit][context]"} }
        mutate { rename => {"[detail]" => "[audit][detail]"} }
        mutate { rename => {"[error_severity]" => "[audit][error_severity]"} }
        mutate { rename => {"[message]" => "[audit][message]"} }
        mutate { rename => {"[host]" => "[audit][server][instance_name]"} }
        mutate { rename => {"[database_name]" => "[audit][database][name]"} }
        mutate { rename => {"[user_name]" => "[audit][database][username]"} }
        mutate { rename => {"[location]" => "[audit][query][location]"} }
        mutate { rename => {"[sql_state_code]" => "[audit][query][state_code]"} }
        mutate { rename => {"[internal_query]" => "[audit][query][internal_query]"} }
        mutate { rename => {"[source]" => "[audit][file][name]"} }
        mutate { rename => {"[offset]" => "[audit][file][offset]"} }


        # use the field session_start_time as timestamp for the event
        mutate { convert => ["session_start_time" , "string"] }
        date { match => ["session_start_time", "YYYY-MM-dd HH:mm:ss 'CEST'"] }
 
        # remove unnecessary fields
        mutate { remove_field => ["[beat][name]","[beat][version]","[beat][hostname]","hint", "virtual_transaction_id","transaction_id","query_pos","query", "process_id","internal_query_pos","input_type","type", "session_line_num","command_tag","log_time","session_start_time", "connection_from"] }
}
output {
        # uncomment the following line for getting a debug output in logstash
        #stdout { codec => rubydebug}

        # write the output to the elasticsearch database
        elasticsearch {
                hosts => ["127.0.0.1:9200"]
                index => "audit_databases_postgresql-%{+YYYY.MM.dd}"
        }
}
